{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "選好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pref(n,m):\n",
    "    row = torch.linspace(1/m, 1, m)  # 1行分の値を生成\n",
    "    pref = torch.stack([row[torch.randperm(m)] for _ in range(n)])\n",
    "    return pref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サイクル１つ見つけたらそこで終了してしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAssignmentAdjuster:\n",
    "    def __init__(self, P, preferences):\n",
    "        \"\"\"\n",
    "        P: n x n の二重確率行列 (torch.Tensor)\n",
    "        preferences: n x n の選好行列 (torch.Tensor)\n",
    "                     各行 i はエージェント i の選好を表し、値が大きいほど好む\n",
    "        \"\"\"\n",
    "        self.P = P\n",
    "        self.preferences = preferences\n",
    "        self.n = P.shape[0]\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        オブジェクトを頂点とするグラフを構築する。\n",
    "        エッジ (a -> b) は、あるエージェント i が a を b より好む（preferences[i, a] > preferences[i, b]）\n",
    "        かつ P[i, b] > 0 である場合に追加する。\n",
    "        エッジには (b, i, P[i,b]) の情報を記録する。\n",
    "        \"\"\"\n",
    "        graph = {a: [] for a in range(self.n)}\n",
    "        for a in range(self.n):\n",
    "            for b in range(self.n):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                for i in range(self.n):\n",
    "                    if self.preferences[i, a] > self.preferences[i, b] and self.P[i, b] > 0:\n",
    "                        graph[a].append((b, i, self.P[i, b].item()))\n",
    "                        # 同じ (a, b) ペアに対しては最初に見つかったエージェントのみ利用\n",
    "                        break\n",
    "        return graph\n",
    "\n",
    "    def find_cycle(self, graph):\n",
    "        \"\"\"\n",
    "        DFS を用いてグラフ中のサイクル（閉路）を探索する。\n",
    "        サイクルが見つかった場合は、サイクルを構成するエッジのリストを返す。\n",
    "        各エッジは (from_object, to_object, witness_agent, available_probability) のタプル。\n",
    "        サイクルがなければ None を返す。\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = []  # 各要素は (vertex, edge_info)。最初の頂点は edge_info=None\n",
    "\n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            rec_stack.append((v, None))\n",
    "            for (nbr, agent, avail) in graph[v]:\n",
    "                # すでに経路に nbr が含まれていればサイクル成立\n",
    "                for idx, (node, _) in enumerate(rec_stack):\n",
    "                    if node == nbr:\n",
    "                        cycle_edges = []\n",
    "                        # rec_stack[idx+1:] に記録されている各エッジ情報がサイクル内のエッジ\n",
    "                        for j in range(idx + 1, len(rec_stack)):\n",
    "                            edge = rec_stack[j][1]\n",
    "                            if edge is not None:\n",
    "                                cycle_edges.append(edge)\n",
    "                        # 現在のエッジ (v -> nbr) を追加\n",
    "                        cycle_edges.append((v, nbr, agent, avail))\n",
    "                        return cycle_edges\n",
    "                if nbr not in visited:\n",
    "                    # 現在のエッジ (v -> nbr) を経路に記録して DFS\n",
    "                    rec_stack.append((v, (v, nbr, agent, avail)))\n",
    "                    result = dfs(nbr)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "                    rec_stack.pop()\n",
    "            rec_stack.pop()\n",
    "            return None\n",
    "\n",
    "        for vertex in range(self.n):\n",
    "            if vertex not in visited:\n",
    "                cycle = dfs(vertex)\n",
    "                if cycle is not None:\n",
    "                    return cycle\n",
    "        return None\n",
    "\n",
    "    def execute_cycle(self):\n",
    "        \"\"\"\n",
    "        グラフを構築し、サイクルを探索する。\n",
    "        サイクルが存在する場合、サイクル内の各エッジで交換可能な最小の確率 epsilon\n",
    "        を算出し、その分だけ、サイクル内の各エッジ (a -> b) に対して、証人エージェント i の行\n",
    "        で、オブジェクト a の割当を epsilon 増加、オブジェクト b の割当を epsilon 減少する。\n",
    "        更新後の二重確率行列 Q、epsilon、サイクル内のエッジ情報を返す。\n",
    "        サイクルが存在しない場合は P をそのまま返す。\n",
    "        \"\"\"\n",
    "        graph = self.build_graph()\n",
    "        cycle = self.find_cycle(graph)\n",
    "        if cycle is None:\n",
    "            print(\"サイクルが存在しません。P は順序効率的です。\")\n",
    "            return self.P, 0.0, None\n",
    "        \n",
    "        # サイクル内の各エッジの available_probability の最小値 epsilon を求める\n",
    "        epsilons = [edge[3] for edge in cycle]\n",
    "        epsilon = min(epsilons)\n",
    "        \n",
    "        # P のコピー Q を作成して更新\n",
    "        Q = self.P.clone()\n",
    "        for (a, b, agent, avail) in cycle:\n",
    "            Q[agent, b] -= epsilon\n",
    "            Q[agent, a] += epsilon\n",
    "        return Q, epsilon, cycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 例: 4エージェント・4オブジェクトの場合\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      4\u001b[0m         [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.0\u001b[39m],\n\u001b[1;32m      5\u001b[0m         [\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.0\u001b[39m],\n\u001b[1;32m      6\u001b[0m         [\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.0\u001b[39m],\n\u001b[1;32m      7\u001b[0m         [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]\n\u001b[1;32m      8\u001b[0m     ])\n\u001b[1;32m     10\u001b[0m     preferences \u001b[38;5;241m=\u001b[39m make_pref(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     12\u001b[0m     adjuster \u001b[38;5;241m=\u001b[39m RandomAssignmentAdjuster(P, preferences)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 例: 4エージェント・4オブジェクトの場合\n",
    "    P = torch.tensor([\n",
    "        [0.5, 0.25, 0.25, 0.0],\n",
    "        [0.25, 0.5, 0.25, 0.0],\n",
    "        [0.25, 0.25, 0.5, 0.0],\n",
    "        [0.0, 0.0, 0.0, 1.0]\n",
    "    ])\n",
    "    \n",
    "    preferences = make_pref(4, 4)\n",
    "    \n",
    "    adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "    \n",
    "    print(\"元の二重確率行列 P:\")\n",
    "    print(P)\n",
    "    print(\"\\n選好行列:\")\n",
    "    print(preferences)\n",
    "    \n",
    "    Q, exchanged, cycle = adjuster.execute_cycle()\n",
    "    \n",
    "    if cycle is not None:\n",
    "        print(\"\\n検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\")\n",
    "        for edge in cycle:\n",
    "            print(edge)\n",
    "        print(\"\\nサイクル内で交換された確率 (epsilon):\", exchanged)\n",
    "        print(\"\\n更新後の二重確率行列 Q:\")\n",
    "        print(Q)\n",
    "    else:\n",
    "        print(\"\\nP はすでに順序効率的です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.5000, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2500, 0.5000, 0.2500, 0.0000],\n",
      "        [0.2500, 0.2500, 0.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "\n",
      "選好行列:\n",
      "tensor([[0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.7500, 1.0000, 0.2500],\n",
      "        [0.2500, 0.7500, 1.0000, 0.5000],\n",
      "        [0.5000, 1.0000, 0.2500, 0.7500]])\n",
      "\n",
      "検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\n",
      "(0, 2, 0, 0.25)\n",
      "(2, 0, 1, 0.25)\n",
      "\n",
      "サイクル内で交換された確率 (epsilon): 0.25\n",
      "\n",
      "更新後の二重確率行列 Q:\n",
      "tensor([[0.7500, 0.2500, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.5000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 例: 4エージェント・4オブジェクトの場合\n",
    "P = torch.tensor([\n",
    "    [0.5, 0.25, 0.25, 0.0],\n",
    "    [0.25, 0.5, 0.25, 0.0],\n",
    "    [0.25, 0.25, 0.5, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "    \n",
    "preferences = make_pref(4, 4)\n",
    "\n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "\n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列:\")\n",
    "print(preferences)\n",
    "\n",
    "Q, exchanged, cycle = adjuster.execute_cycle()\n",
    "\n",
    "if cycle is not None:\n",
    "    print(\"\\n検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\")\n",
    "    for edge in cycle:\n",
    "        print(edge)\n",
    "    print(\"\\nサイクル内で交換された確率 (epsilon):\", exchanged)\n",
    "    print(\"\\n更新後の二重確率行列 Q:\")\n",
    "    print(Q)\n",
    "else:\n",
    "    print(\"\\nP はすでに順序効率的です。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "３人も対応している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.4000, 0.3000, 0.0000, 0.3000],\n",
      "        [0.0000, 0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000, 0.0000],\n",
      "        [0.3000, 0.0000, 0.3000, 0.4000]])\n",
      "\n",
      "選好行列:\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.7500, 0.5000, 1.0000, 0.2500],\n",
      "        [0.7500, 0.5000, 0.2500, 1.0000]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomAssignmentAdjuster' object has no attribute 'execute_cycle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m選好行列:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(preferences)\n\u001b[0;32m---> 22\u001b[0m Q, exchanged, cycle \u001b[38;5;241m=\u001b[39m \u001b[43madjuster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_cycle\u001b[49m()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cycle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomAssignmentAdjuster' object has no attribute 'execute_cycle'"
     ]
    }
   ],
   "source": [
    "P = torch.tensor([\n",
    "        [0.4, 0.3, 0.0, 0.3],\n",
    "        [0.0, 0.4, 0.3, 0.3],\n",
    "        [0.3, 0.3, 0.4, 0.0],\n",
    "        [0.3, 0.0, 0.3, 0.4]\n",
    "    ])\n",
    "\n",
    "preferences = torch.tensor([\n",
    "        [1.00, 0.75, 0.5,  0.25],   # Agent0\n",
    "        [0.75, 1.00, 0.5,  0.25],   # Agent1\n",
    "        [0.75, 0.5,  1.00, 0.25],   # Agent2\n",
    "        [0.75, 0.5,  0.25, 1.00]     # Agent3\n",
    "    ])\n",
    "\n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "\n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列:\")\n",
    "print(preferences)\n",
    "\n",
    "Q, exchanged, cycle = adjuster.execute_cycle()\n",
    "\n",
    "if cycle is not None:\n",
    "    print(\"\\n検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\")\n",
    "    for edge in cycle:\n",
    "        print(edge)\n",
    "    print(\"\\nサイクル内で交換された確率 (epsilon):\", exchanged)\n",
    "    print(\"\\n更新後の二重確率行列 Q:\")\n",
    "    print(Q)\n",
    "else:\n",
    "    print(\"\\nP はすでに順序効率的です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.4000, 0.3000, 0.2000, 0.1000],\n",
      "        [0.1000, 0.4000, 0.3000, 0.2000],\n",
      "        [0.2000, 0.1000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.2000, 0.1000, 0.4000]])\n",
      "\n",
      "選好行列:\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\n",
      "(0, 1, 0, 0.30000001192092896)\n",
      "(1, 0, 1, 0.10000000149011612)\n",
      "\n",
      "サイクル内で交換された確率 (epsilon): 0.10000000149011612\n",
      "\n",
      "更新後の二重確率行列 Q:\n",
      "tensor([[0.5000, 0.2000, 0.2000, 0.1000],\n",
      "        [0.0000, 0.5000, 0.3000, 0.2000],\n",
      "        [0.2000, 0.1000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.2000, 0.1000, 0.4000]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.tensor([\n",
    "        [0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.4, 0.3, 0.2],\n",
    "        [0.2, 0.1, 0.4, 0.3],\n",
    "        [0.3, 0.2, 0.1, 0.4]\n",
    "    ])\n",
    "\n",
    "preferences = torch.tensor([\n",
    "        [1.00, 0.75, 0.5,  0.25],   # エージェント0\n",
    "        [0.75, 1.00, 0.5,  0.25],   # エージェント1\n",
    "        [0.5,  0.25, 1.00, 0.75],   # エージェント2\n",
    "        [0.25, 0.5,  0.75, 1.00]     # エージェント3\n",
    "    ])\n",
    "\n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "\n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列:\")\n",
    "print(preferences)\n",
    "\n",
    "Q, exchanged, cycle = adjuster.execute_cycle()\n",
    "\n",
    "if cycle is not None:\n",
    "    print(\"\\n検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\")\n",
    "    for edge in cycle:\n",
    "        print(edge)\n",
    "    print(\"\\nサイクル内で交換された確率 (epsilon):\", exchanged)\n",
    "    print(\"\\n更新後の二重確率行列 Q:\")\n",
    "    print(Q)\n",
    "else:\n",
    "    print(\"\\nP はすでに順序効率的です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.7000, 0.3000, 0.0000, 0.0000],\n",
      "        [0.1000, 0.4000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.2000, 0.4000, 0.4000],\n",
      "        [0.2000, 0.1000, 0.1000, 0.6000]])\n",
      "\n",
      "選好行列:\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.5000, 1.0000, 0.7500, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\n",
      "(0, 1, 0, 0.30000001192092896)\n",
      "(1, 0, 1, 0.10000000149011612)\n",
      "\n",
      "サイクル内で交換された確率 (epsilon): 0.10000000149011612\n",
      "\n",
      "更新後の二重確率行列 Q:\n",
      "tensor([[0.8000, 0.2000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.2000, 0.4000, 0.4000],\n",
      "        [0.2000, 0.1000, 0.1000, 0.6000]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.tensor([\n",
    "        [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "        [0.1, 0.4, 0.5, 0.0],  # Agent1\n",
    "        [0.0, 0.2, 0.4, 0.4],  # Agent2\n",
    "        [0.2, 0.1, 0.1, 0.6]   # Agent3\n",
    "    ])\n",
    "\n",
    "preferences = torch.tensor([\n",
    "        [1.00, 0.75, 0.5,  0.25],  # Agent0: object0 > object1 > object2 > object3\n",
    "        [0.50, 1.00, 0.75, 0.25],   # Agent1: object1 > object2 > object0 > object3\n",
    "        [0.50, 0.25, 1.00, 0.75],   # Agent2: object2 > object3 > object0 > object1  ※edge (object2→object3)用\n",
    "        [0.25, 0.50, 0.75, 1.00]    # Agent3: object3 > object0 > object1 > object2  ※edge (object3→object0)用\n",
    "    ])\n",
    "\n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "\n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列:\")\n",
    "print(preferences)\n",
    "\n",
    "Q, exchanged, cycle = adjuster.execute_cycle()\n",
    "\n",
    "if cycle is not None:\n",
    "    print(\"\\n検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\")\n",
    "    for edge in cycle:\n",
    "        print(edge)\n",
    "    print(\"\\nサイクル内で交換された確率 (epsilon):\", exchanged)\n",
    "    print(\"\\n更新後の二重確率行列 Q:\")\n",
    "    print(Q)\n",
    "else:\n",
    "    print(\"\\nP はすでに順序効率的です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.7000, 0.3000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.7000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6000, 0.4000],\n",
      "        [0.0000, 0.0000, 0.4000, 0.6000]])\n",
      "\n",
      "選好行列:\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\n",
      "(0, 1, 0, 0.30000001192092896)\n",
      "(1, 0, 1, 0.30000001192092896)\n",
      "\n",
      "サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "\n",
      "更新後の二重確率行列 Q:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6000, 0.4000],\n",
      "        [0.0000, 0.0000, 0.4000, 0.6000]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.tensor([\n",
    "        [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "        [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "        [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "        [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ])\n",
    "\n",
    "preferences = torch.tensor([\n",
    "        [1.00, 0.75, 0.5,  0.25], \n",
    "        [0.75, 1.00, 0.5, 0.25], \n",
    "        [0.50, 0.25, 1.00, 0.75], \n",
    "        [0.25, 0.50, 0.75, 1.00]\n",
    "    ])\n",
    "\n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "\n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列:\")\n",
    "print(preferences)\n",
    "\n",
    "Q, exchanged, cycle = adjuster.execute_cycle()\n",
    "\n",
    "if cycle is not None:\n",
    "    print(\"\\n検出されたサイクル (各タプルは (from_object, to_object, witness_agent, available_probability)):\")\n",
    "    for edge in cycle:\n",
    "        print(edge)\n",
    "    print(\"\\nサイクル内で交換された確率 (epsilon):\", exchanged)\n",
    "    print(\"\\n更新後の二重確率行列 Q:\")\n",
    "    print(Q)\n",
    "else:\n",
    "    print(\"\\nP はすでに順序効率的です。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数サイクル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.7000, 0.3000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.7000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6000, 0.4000],\n",
      "        [0.0000, 0.0000, 0.4000, 0.6000]])\n",
      "\n",
      "選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "各サイクルでの交換内容:\n",
      "【サイクル 1】\n",
      "   (0, 1, 0, 0.30000001192092896)\n",
      "   (1, 0, 1, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "【サイクル 2】\n",
      "   (2, 3, 2, 0.4000000059604645)\n",
      "   (3, 2, 3, 0.4000000059604645)\n",
      "  サイクル内で交換された確率 (epsilon): 0.4000000059604645\n",
      "\n",
      "最終的な更新後の二重確率行列 Q:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class RandomAssignmentAdjuster:\n",
    "    def __init__(self, P, preferences):\n",
    "        \"\"\"\n",
    "        P: n×n の二重確率行列 (torch.Tensor)\n",
    "        preferences: n×n の選好行列 (torch.Tensor)\n",
    "                     各行 i はエージェント i の選好を表し、値が大きいほど好む\n",
    "        \"\"\"\n",
    "        self.P = P.clone()\n",
    "        self.preferences = preferences\n",
    "        self.n = P.shape[0]\n",
    "\n",
    "    def build_graph(self, Q):\n",
    "        \"\"\"\n",
    "        Q: 現在の二重確率行列 (torch.Tensor)\n",
    "        オブジェクトを頂点とするグラフを構築する。\n",
    "        エッジ (a -> b) は、あるエージェント i が a を b より好む（preferences[i, a] > preferences[i, b]）\n",
    "        かつ Q[i, b] > 0 である場合に追加する。\n",
    "        エッジには (b, i, Q[i, b]) の情報を記録する。\n",
    "        \"\"\"\n",
    "        graph = {a: [] for a in range(self.n)}\n",
    "        for a in range(self.n):\n",
    "            for b in range(self.n):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                for i in range(self.n):\n",
    "                    if self.preferences[i, a] > self.preferences[i, b] and Q[i, b] > 0:\n",
    "                        graph[a].append((b, i, Q[i, b].item()))\n",
    "                        # 同じ (a, b) ペアについて、最初に条件を満たしたエージェントを witness とする\n",
    "                        break\n",
    "        return graph\n",
    "\n",
    "    def find_cycle(self, graph):\n",
    "        \"\"\"\n",
    "        DFS を用いてグラフ中のサイクル（閉路）を探索する。\n",
    "        サイクルが見つかった場合は、サイクルを構成するエッジのリストを返す。\n",
    "        各エッジは (from_object, to_object, witness_agent, available_probability) のタプル。\n",
    "        サイクルがなければ None を返す。\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = []  # 各要素は (vertex, edge_info)。最初の頂点は edge_info=None\n",
    "\n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            rec_stack.append((v, None))\n",
    "            for (nbr, agent, avail) in graph[v]:\n",
    "                for idx, (node, _) in enumerate(rec_stack):\n",
    "                    if node == nbr:\n",
    "                        cycle_edges = []\n",
    "                        # rec_stack[idx+1:] に記録されているエッジ情報がサイクル内のエッジ\n",
    "                        for j in range(idx + 1, len(rec_stack)):\n",
    "                            edge = rec_stack[j][1]\n",
    "                            if edge is not None:\n",
    "                                cycle_edges.append(edge)\n",
    "                        cycle_edges.append((v, nbr, agent, avail))\n",
    "                        return cycle_edges\n",
    "                if nbr not in visited:\n",
    "                    rec_stack.append((v, (v, nbr, agent, avail)))\n",
    "                    result = dfs(nbr)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "                    rec_stack.pop()\n",
    "            rec_stack.pop()\n",
    "            return None\n",
    "\n",
    "        for vertex in range(self.n):\n",
    "            if vertex not in visited:\n",
    "                cycle = dfs(vertex)\n",
    "                if cycle is not None:\n",
    "                    return cycle\n",
    "        return None\n",
    "\n",
    "    def execute_all_cycles(self):\n",
    "        \"\"\"\n",
    "        現在の二重確率行列 self.P に対して、グラフを構築し、サイクルを探索しては\n",
    "        そのサイクル内で交換可能な最小の確率 epsilon だけ交換を実施する。\n",
    "        複数のサイクルが存在する場合、すべてのサイクルで交換が行われるまで反復する。\n",
    "        交換が行われたサイクルと各サイクルでの epsilon を記録し、最終的な更新後行列 Q と\n",
    "        サイクル情報のリストを返す。\n",
    "        \"\"\"\n",
    "        Q = self.P.clone()\n",
    "        cycles_exchanges = []\n",
    "        while True:\n",
    "            graph = self.build_graph(Q)\n",
    "            cycle = self.find_cycle(graph)\n",
    "            if cycle is None:\n",
    "                break\n",
    "            # サイクル内の各エッジの利用可能な確率の最小値を epsilon とする\n",
    "            epsilons = [edge[3] for edge in cycle]\n",
    "            epsilon = min(epsilons)\n",
    "            # サイクル内の各エッジについて交換を実施\n",
    "            for (a, b, agent, avail) in cycle:\n",
    "                Q[agent, b] -= epsilon\n",
    "                Q[agent, a] += epsilon\n",
    "            cycles_exchanges.append((cycle, epsilon))\n",
    "        return Q, cycles_exchanges\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 例：2つの2人サイクルが存在するケース（4エージェント・4オブジェクト）\n",
    "    P = torch.tensor([\n",
    "        [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "        [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "        [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "        [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ])\n",
    "    \n",
    "    # 各行は {1, 0.75, 0.5, 0.25} の順列\n",
    "    preferences = torch.tensor([\n",
    "        [1.00, 0.75, 0.50, 0.25],\n",
    "        [0.75, 1.00, 0.50, 0.25],\n",
    "        [0.50, 0.25, 1.00, 0.75],\n",
    "        [0.25, 0.50, 0.75, 1.00]\n",
    "    ])\n",
    "    \n",
    "    adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "    \n",
    "    print(\"元の二重確率行列 P:\")\n",
    "    print(P)\n",
    "    print(\"\\n選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\")\n",
    "    print(preferences)\n",
    "    \n",
    "    Q, cycles_exchanges = adjuster.execute_all_cycles()\n",
    "    \n",
    "    if cycles_exchanges:\n",
    "        print(\"\\n各サイクルでの交換内容:\")\n",
    "        for idx, (cycle, epsilon) in enumerate(cycles_exchanges):\n",
    "            print(f\"【サイクル {idx+1}】\")\n",
    "            for edge in cycle:\n",
    "                print(\"  \", edge)\n",
    "            print(\"  サイクル内で交換された確率 (epsilon):\", epsilon)\n",
    "    else:\n",
    "        print(\"\\nサイクルは検出されませんでした。\")\n",
    "    \n",
    "    print(\"\\n最終的な更新後の二重確率行列 Q:\")\n",
    "    print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各サイクル内の和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.7000, 0.3000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.7000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6000, 0.4000],\n",
      "        [0.0000, 0.0000, 0.4000, 0.6000]])\n",
      "\n",
      "選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "各サイクルでの交換内容:\n",
      "【サイクル 1】\n",
      "   (0, 1, 0, 0.30000001192092896)\n",
      "   (1, 0, 1, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "  サイクル内の交換確率の和: 0.6000000238418579\n",
      "【サイクル 2】\n",
      "   (2, 3, 2, 0.4000000059604645)\n",
      "   (3, 2, 3, 0.4000000059604645)\n",
      "  サイクル内で交換された確率 (epsilon): 0.4000000059604645\n",
      "  サイクル内の交換確率の和: 0.800000011920929\n",
      "\n",
      "最終的な更新後の二重確率行列 Q:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class RandomAssignmentAdjuster:\n",
    "    def __init__(self, P, preferences):\n",
    "        \"\"\"\n",
    "        P: n×n の二重確率行列 (torch.Tensor)\n",
    "        preferences: n×n の選好行列 (torch.Tensor)\n",
    "                     各行 i はエージェント i の選好を表し、値が大きいほど好む\n",
    "        \"\"\"\n",
    "        self.P = P.clone()\n",
    "        self.preferences = preferences\n",
    "        self.n = P.shape[0]\n",
    "\n",
    "    def build_graph(self, Q):\n",
    "        \"\"\"\n",
    "        Q: 現在の二重確率行列 (torch.Tensor)\n",
    "        オブジェクトを頂点とするグラフを構築する。\n",
    "        エッジ (a -> b) は、あるエージェント i が a を b より好む（preferences[i, a] > preferences[i, b]）\n",
    "        かつ Q[i, b] > 0 である場合に追加する。\n",
    "        エッジには (b, i, Q[i, b]) の情報を記録する。\n",
    "        \"\"\"\n",
    "        graph = {a: [] for a in range(self.n)}\n",
    "        for a in range(self.n):\n",
    "            for b in range(self.n):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                for i in range(self.n):\n",
    "                    if self.preferences[i, a] > self.preferences[i, b] and Q[i, b] > 0:\n",
    "                        graph[a].append((b, i, Q[i, b].item()))\n",
    "                        # 同じ (a, b) ペアについて、最初に条件を満たしたエージェントを witness とする\n",
    "                        break\n",
    "        return graph\n",
    "\n",
    "    def find_cycle(self, graph):\n",
    "        \"\"\"\n",
    "        DFS を用いてグラフ中のサイクル（閉路）を探索する。\n",
    "        サイクルが見つかった場合は、サイクルを構成するエッジのリストを返す。\n",
    "        各エッジは (from_object, to_object, witness_agent, available_probability) のタプル。\n",
    "        サイクルがなければ None を返す。\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = []  # 各要素は (vertex, edge_info)。最初の頂点は edge_info=None\n",
    "\n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            rec_stack.append((v, None))\n",
    "            for (nbr, agent, avail) in graph[v]:\n",
    "                for idx, (node, _) in enumerate(rec_stack):\n",
    "                    if node == nbr:\n",
    "                        cycle_edges = []\n",
    "                        # rec_stack[idx+1:] に記録されているエッジ情報がサイクル内のエッジ\n",
    "                        for j in range(idx + 1, len(rec_stack)):\n",
    "                            edge = rec_stack[j][1]\n",
    "                            if edge is not None:\n",
    "                                cycle_edges.append(edge)\n",
    "                        cycle_edges.append((v, nbr, agent, avail))\n",
    "                        return cycle_edges\n",
    "                if nbr not in visited:\n",
    "                    rec_stack.append((v, (v, nbr, agent, avail)))\n",
    "                    result = dfs(nbr)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "                    rec_stack.pop()\n",
    "            rec_stack.pop()\n",
    "            return None\n",
    "\n",
    "        for vertex in range(self.n):\n",
    "            if vertex not in visited:\n",
    "                cycle = dfs(vertex)\n",
    "                if cycle is not None:\n",
    "                    return cycle\n",
    "        return None\n",
    "\n",
    "    def execute_all_cycles(self):\n",
    "        \"\"\"\n",
    "        現在の二重確率行列 self.P に対して、グラフを構築し、サイクルを探索しては\n",
    "        そのサイクル内で交換可能な最小の確率 epsilon だけ交換を実施する。\n",
    "        複数のサイクルが存在する場合、すべてのサイクルで交換が行われるまで反復する。\n",
    "        各サイクルで、サイクル内のエッジ数に epsilon を掛けた「交換された確率の和」も計算する。\n",
    "        最終的な更新後行列 Q と、各サイクルのサイクル情報、epsilon、交換和のタプルのリストを返す。\n",
    "        \"\"\"\n",
    "        Q = self.P.clone()\n",
    "        cycles_exchanges = []\n",
    "        while True:\n",
    "            graph = self.build_graph(Q)\n",
    "            cycle = self.find_cycle(graph)\n",
    "            if cycle is None:\n",
    "                break\n",
    "            # サイクル内の各エッジの利用可能な確率の最小値を epsilon とする\n",
    "            epsilons = [edge[3] for edge in cycle]\n",
    "            epsilon = min(epsilons)\n",
    "            # サイクル内で交換された確率の和は、サイクル内のエッジ数 * epsilon\n",
    "            sum_exchanged = len(cycle) * epsilon\n",
    "            # サイクル内の各エッジについて交換を実施\n",
    "            for (a, b, agent, avail) in cycle:\n",
    "                Q[agent, b] -= epsilon\n",
    "                Q[agent, a] += epsilon\n",
    "            cycles_exchanges.append((cycle, epsilon, sum_exchanged))\n",
    "        return Q, cycles_exchanges\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 例：2つの2人サイクルが存在するケース（4エージェント・4オブジェクト）\n",
    "    P = torch.tensor([\n",
    "        [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "        [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "        [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "        [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ])\n",
    "    \n",
    "    # 各行は {1, 0.75, 0.5, 0.25} の順列\n",
    "    preferences = torch.tensor([\n",
    "        [1.00, 0.75, 0.50, 0.25],\n",
    "        [0.75, 1.00, 0.50, 0.25],\n",
    "        [0.50, 0.25, 1.00, 0.75],\n",
    "        [0.25, 0.50, 0.75, 1.00]\n",
    "    ])\n",
    "    \n",
    "    adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "    \n",
    "    print(\"元の二重確率行列 P:\")\n",
    "    print(P)\n",
    "    print(\"\\n選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\")\n",
    "    print(preferences)\n",
    "    \n",
    "    Q, cycles_exchanges = adjuster.execute_all_cycles()\n",
    "    \n",
    "    if cycles_exchanges:\n",
    "        print(\"\\n各サイクルでの交換内容:\")\n",
    "        for idx, (cycle, epsilon, sum_exchanged) in enumerate(cycles_exchanges):\n",
    "            print(f\"【サイクル {idx+1}】\")\n",
    "            for edge in cycle:\n",
    "                print(\"  \", edge)\n",
    "            print(\"  サイクル内で交換された確率 (epsilon):\", epsilon)\n",
    "            print(\"  サイクル内の交換確率の和:\", sum_exchanged)\n",
    "    else:\n",
    "        print(\"\\nサイクルは検出されませんでした。\")\n",
    "    \n",
    "    print(\"\\n最終的な更新後の二重確率行列 Q:\")\n",
    "    print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サイクル同士の和を考える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAssignmentAdjuster:\n",
    "    def __init__(self, P, preferences):\n",
    "        \"\"\"\n",
    "        P: n×n の二重確率行列 (torch.Tensor)\n",
    "        preferences: n×n の選好行列 (torch.Tensor)\n",
    "                     各行 i はエージェント i の選好を表し、値が大きいほど好む\n",
    "        \"\"\"\n",
    "        self.P = P.clone()\n",
    "        self.preferences = preferences\n",
    "        self.n = P.shape[0]\n",
    "\n",
    "    def build_graph(self, Q):\n",
    "        \"\"\"\n",
    "        Q: 現在の二重確率行列 (torch.Tensor)\n",
    "        オブジェクトを頂点とするグラフを構築する。\n",
    "        エッジ (a -> b) は、あるエージェント i が a を b より好む（preferences[i, a] > preferences[i, b]）\n",
    "        かつ Q[i, b] > 0 である場合に追加する。\n",
    "        エッジには (b, i, Q[i, b]) の情報を記録する。\n",
    "        \"\"\"\n",
    "        graph = {a: [] for a in range(self.n)}\n",
    "        for a in range(self.n):\n",
    "            for b in range(self.n):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                for i in range(self.n):\n",
    "                    if self.preferences[i, a] > self.preferences[i, b] and Q[i, b] > 0:\n",
    "                        graph[a].append((b, i, Q[i, b].item()))\n",
    "                        # 同じ (a, b) ペアについて、最初に条件を満たしたエージェントを witness とする\n",
    "                        break\n",
    "        return graph\n",
    "\n",
    "    def find_cycle(self, graph):\n",
    "        \"\"\"\n",
    "        DFS を用いてグラフ中のサイクル（閉路）を探索する。\n",
    "        サイクルが見つかった場合は、サイクルを構成するエッジのリストを返す。\n",
    "        各エッジは (from_object, to_object, witness_agent, available_probability) のタプル。\n",
    "        サイクルがなければ None を返す。\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = []  # 各要素は (vertex, edge_info)。最初の頂点は edge_info=None\n",
    "\n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            rec_stack.append((v, None))\n",
    "            for (nbr, agent, avail) in graph[v]:\n",
    "                for idx, (node, _) in enumerate(rec_stack):\n",
    "                    if node == nbr:\n",
    "                        cycle_edges = []\n",
    "                        # rec_stack[idx+1:] に記録されているエッジ情報がサイクル内のエッジ\n",
    "                        for j in range(idx + 1, len(rec_stack)):\n",
    "                            edge = rec_stack[j][1]\n",
    "                            if edge is not None:\n",
    "                                cycle_edges.append(edge)\n",
    "                        cycle_edges.append((v, nbr, agent, avail))\n",
    "                        return cycle_edges\n",
    "                if nbr not in visited:\n",
    "                    rec_stack.append((v, (v, nbr, agent, avail)))\n",
    "                    result = dfs(nbr)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "                    rec_stack.pop()\n",
    "            rec_stack.pop()\n",
    "            return None\n",
    "\n",
    "        for vertex in range(self.n):\n",
    "            if vertex not in visited:\n",
    "                cycle = dfs(vertex)\n",
    "                if cycle is not None:\n",
    "                    return cycle\n",
    "        return None\n",
    "\n",
    "    def execute_all_cycles(self):\n",
    "        \"\"\"\n",
    "        現在の二重確率行列 self.P に対して、グラフを構築し、サイクルを探索しては\n",
    "        そのサイクル内で交換可能な最小の確率 epsilon だけ交換を実施する。\n",
    "        複数のサイクルが存在する場合、すべてのサイクルで交換が行われるまで反復する。\n",
    "        交換が行われたサイクルと各サイクルでの epsilon を記録し、最終的な更新後行列 Q と\n",
    "        サイクル情報のリストを返す。\n",
    "        \"\"\"\n",
    "        Q = self.P.clone()\n",
    "        cycles_exchanges = []\n",
    "        sum_epsilon = 0.0\n",
    "        while True:\n",
    "            graph = self.build_graph(Q)\n",
    "            cycle = self.find_cycle(graph)\n",
    "            if cycle is None:\n",
    "                break\n",
    "            # サイクル内の各エッジの利用可能な確率の最小値を epsilon とする\n",
    "            epsilons = [edge[3] for edge in cycle]\n",
    "            epsilon = min(epsilons)\n",
    "            sum_epsilon += epsilon\n",
    "            # サイクル内の各エッジについて交換を実施\n",
    "            for (a, b, agent, avail) in cycle:\n",
    "                Q[agent, b] -= epsilon\n",
    "                Q[agent, a] += epsilon\n",
    "            cycles_exchanges.append((cycle, epsilon))\n",
    "        return Q, cycles_exchanges, sum_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上手くいってそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.7000, 0.3000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.7000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6000, 0.4000],\n",
      "        [0.0000, 0.0000, 0.4000, 0.6000]])\n",
      "\n",
      "選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "各サイクルでの交換内容:\n",
      "【サイクル 1】\n",
      "   (0, 1, 0, 0.30000001192092896)\n",
      "   (1, 0, 1, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "【サイクル 2】\n",
      "   (2, 3, 2, 0.4000000059604645)\n",
      "   (3, 2, 3, 0.4000000059604645)\n",
      "  サイクル内で交換された確率 (epsilon): 0.4000000059604645\n",
      "\n",
      "交換された確率の和: 0.7000000178813934\n",
      "\n",
      "最終的な更新後の二重確率行列 Q:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 例：2つの2人サイクルが存在するケース（4エージェント・4オブジェクト）\n",
    "P = torch.tensor([\n",
    "    [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "    [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "    [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ])\n",
    "    \n",
    "# 各行は {1, 0.75, 0.5, 0.25} の順列\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.50, 0.25, 1.00, 0.75],\n",
    "    [0.25, 0.50, 0.75, 1.00]\n",
    "    ])\n",
    "    \n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "    \n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\")\n",
    "print(preferences)\n",
    "    \n",
    "Q, cycles_exchanges, sum_epsilon = adjuster.execute_all_cycles()\n",
    "    \n",
    "if cycles_exchanges:\n",
    "    print(\"\\n各サイクルでの交換内容:\")\n",
    "    for idx, (cycle, epsilon) in enumerate(cycles_exchanges):\n",
    "        print(f\"【サイクル {idx+1}】\")\n",
    "        for edge in cycle:\n",
    "            print(\"  \", edge)\n",
    "        print(\"  サイクル内で交換された確率 (epsilon):\", epsilon)\n",
    "    print(\"\\n交換された確率の和:\", sum_epsilon)\n",
    "else:\n",
    "     print(\"\\nサイクルは検出されませんでした。\")\n",
    "\n",
    "print(\"\\n最終的な更新後の二重確率行列 Q:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.4000, 0.3000, 0.0000, 0.3000],\n",
      "        [0.0000, 0.4000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.4000, 0.0000],\n",
      "        [0.3000, 0.0000, 0.3000, 0.4000]])\n",
      "\n",
      "選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.7500, 0.5000, 1.0000, 0.2500],\n",
      "        [0.7500, 0.5000, 0.2500, 1.0000]])\n",
      "\n",
      "各サイクルでの交換内容:\n",
      "【サイクル 1】\n",
      "   (0, 1, 0, 0.30000001192092896)\n",
      "   (1, 2, 1, 0.30000001192092896)\n",
      "   (2, 0, 2, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "【サイクル 2】\n",
      "   (1, 2, 3, 0.30000001192092896)\n",
      "   (2, 1, 2, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "【サイクル 3】\n",
      "   (0, 1, 3, 0.30000001192092896)\n",
      "   (1, 3, 0, 0.30000001192092896)\n",
      "   (3, 0, 3, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "【サイクル 4】\n",
      "   (0, 1, 0, 0.30000001192092896)\n",
      "   (1, 3, 1, 0.30000001192092896)\n",
      "   (3, 0, 3, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "\n",
      "交換された確率の和: 1.2000000476837158\n",
      "\n",
      "最終的な更新後の二重確率行列 Q:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.tensor([\n",
    "        [0.4, 0.3, 0.0, 0.3],\n",
    "        [0.0, 0.4, 0.3, 0.3],\n",
    "        [0.3, 0.3, 0.4, 0.0],\n",
    "        [0.3, 0.0, 0.3, 0.4]\n",
    "    ])\n",
    "\n",
    "preferences = torch.tensor([\n",
    "        [1.00, 0.75, 0.5,  0.25],   # Agent0\n",
    "        [0.75, 1.00, 0.5,  0.25],   # Agent1\n",
    "        [0.75, 0.5,  1.00, 0.25],   # Agent2\n",
    "        [0.75, 0.5,  0.25, 1.00]     # Agent3\n",
    "    ])\n",
    "\n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "    \n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\")\n",
    "print(preferences)\n",
    "    \n",
    "Q, cycles_exchanges, sum_epsilon = adjuster.execute_all_cycles()\n",
    "    \n",
    "if cycles_exchanges:\n",
    "    print(\"\\n各サイクルでの交換内容:\")\n",
    "    for idx, (cycle, epsilon) in enumerate(cycles_exchanges):\n",
    "        print(f\"【サイクル {idx+1}】\")\n",
    "        for edge in cycle:\n",
    "            print(\"  \", edge)\n",
    "        print(\"  サイクル内で交換された確率 (epsilon):\", epsilon)\n",
    "    print(\"\\n交換された確率の和:\", sum_epsilon)\n",
    "else:\n",
    "     print(\"\\nサイクルは検出されませんでした。\")\n",
    "\n",
    "print(\"\\n最終的な更新後の二重確率行列 Q:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if文なくてもちゃんと出力されるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "\n",
      "選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "各サイクルでの交換内容:\n",
      "\n",
      "交換された確率の和: 0.0\n",
      "\n",
      "交換された確率の和: 0.0\n",
      "\n",
      "最終的な更新後の二重確率行列 Q:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 例：2つの2人サイクルが存在するケース（4エージェント・4オブジェクト）\n",
    "P = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # Agent0\n",
    "    [0.0, 1.0, 0.0, 0.0],  # Agent1\n",
    "    [0.0, 0.0, 1.0, 0.0],  # Agent2\n",
    "    [0.0, 0.0, 0.0, 1.0]   # Agent3\n",
    "    ])\n",
    "    \n",
    "# 各行は {1, 0.75, 0.5, 0.25} の順列\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.50, 0.25, 1.00, 0.75],\n",
    "    [0.25, 0.50, 0.75, 1.00]\n",
    "    ])\n",
    "    \n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "    \n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\")\n",
    "print(preferences)\n",
    "    \n",
    "Q, cycles_exchanges, sum_epsilon = adjuster.execute_all_cycles()\n",
    "    \n",
    "\n",
    "print(\"\\n各サイクルでの交換内容:\")\n",
    "for idx, (cycle, epsilon) in enumerate(cycles_exchanges):\n",
    "    print(f\"【サイクル {idx+1}】\")\n",
    "    for edge in cycle:\n",
    "        print(\"  \", edge)\n",
    "    print(\"  サイクル内で交換された確率 (epsilon):\", epsilon)\n",
    "print(\"\\n交換された確率の和:\", sum_epsilon)\n",
    "\n",
    "print(\"\\n最終的な更新後の二重確率行列 Q:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の二重確率行列 P:\n",
      "tensor([[0.7000, 0.3000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.7000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6000, 0.4000],\n",
      "        [0.0000, 0.0000, 0.4000, 0.6000]])\n",
      "\n",
      "選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\n",
      "tensor([[1.0000, 0.7500, 0.5000, 0.2500],\n",
      "        [0.7500, 1.0000, 0.5000, 0.2500],\n",
      "        [0.5000, 0.2500, 1.0000, 0.7500],\n",
      "        [0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "\n",
      "各サイクルでの交換内容:\n",
      "【サイクル 1】\n",
      "   (0, 1, 0, 0.30000001192092896)\n",
      "   (1, 0, 1, 0.30000001192092896)\n",
      "  サイクル内で交換された確率 (epsilon): 0.30000001192092896\n",
      "【サイクル 2】\n",
      "   (2, 3, 2, 0.4000000059604645)\n",
      "   (3, 2, 3, 0.4000000059604645)\n",
      "  サイクル内で交換された確率 (epsilon): 0.4000000059604645\n",
      "\n",
      "交換された確率の和: 0.7000000178813934\n",
      "\n",
      "最終的な更新後の二重確率行列 Q:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 例：2つの2人サイクルが存在するケース（4エージェント・4オブジェクト）\n",
    "P = torch.tensor([\n",
    "    [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "    [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "    [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ])\n",
    "    \n",
    "# 各行は {1, 0.75, 0.5, 0.25} の順列\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.50, 0.25, 1.00, 0.75],\n",
    "    [0.25, 0.50, 0.75, 1.00]\n",
    "    ])\n",
    "\n",
    "adjuster = RandomAssignmentAdjuster(P, preferences)\n",
    "    \n",
    "print(\"元の二重確率行列 P:\")\n",
    "print(P)\n",
    "print(\"\\n選好行列 (各行は [1, 0.75, 0.5, 0.25] の順列):\")\n",
    "print(preferences)\n",
    "    \n",
    "Q, cycles_exchanges, sum_epsilon = adjuster.execute_all_cycles()\n",
    "    \n",
    "\n",
    "print(\"\\n各サイクルでの交換内容:\")\n",
    "for idx, (cycle, epsilon) in enumerate(cycles_exchanges):\n",
    "    print(f\"【サイクル {idx+1}】\")\n",
    "    for edge in cycle:\n",
    "        print(\"  \", edge)\n",
    "    print(\"  サイクル内で交換された確率 (epsilon):\", epsilon)\n",
    "print(\"\\n交換された確率の和:\", sum_epsilon)\n",
    "\n",
    "print(\"\\n最終的な更新後の二重確率行列 Q:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交換された確率の和のみ出力されるように"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compute_ev:\n",
    "    def __init__(self, P, preferences):\n",
    "        \"\"\"\n",
    "        P: n×n の二重確率行列 (torch.Tensor)\n",
    "        preferences: n×n の選好行列 (torch.Tensor)\n",
    "                     各行 i はエージェント i の選好を表し、値が大きいほど好む\n",
    "        \"\"\"\n",
    "        self.P = P.clone()\n",
    "        self.preferences = preferences\n",
    "        self.n = P.shape[0]\n",
    "\n",
    "    def build_graph(self, Q):\n",
    "        \"\"\"\n",
    "        Q: 現在の二重確率行列 (torch.Tensor)\n",
    "        オブジェクトを頂点とするグラフを構築する。\n",
    "        エッジ (a -> b) は、あるエージェント i が a を b より好む（preferences[i, a] > preferences[i, b]）\n",
    "        かつ Q[i, b] > 0 である場合に追加する。\n",
    "        エッジには (b, i, Q[i, b]) の情報を記録する。\n",
    "        \"\"\"\n",
    "        graph = {a: [] for a in range(self.n)}\n",
    "        for a in range(self.n):\n",
    "            for b in range(self.n):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                for i in range(self.n):\n",
    "                    if self.preferences[i, a] > self.preferences[i, b] and Q[i, b] > 0:\n",
    "                        graph[a].append((b, i, Q[i, b].item()))\n",
    "                        # 同じ (a, b) ペアについて、最初に条件を満たしたエージェントを witness とする\n",
    "                        break\n",
    "        return graph\n",
    "\n",
    "    def find_cycle(self, graph):\n",
    "        \"\"\"\n",
    "        DFS を用いてグラフ中のサイクル（閉路）を探索する。\n",
    "        サイクルが見つかった場合は、サイクルを構成するエッジのリストを返す。\n",
    "        各エッジは (from_object, to_object, witness_agent, available_probability) のタプル。\n",
    "        サイクルがなければ None を返す。\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = []  # 各要素は (vertex, edge_info)。最初の頂点は edge_info=None\n",
    "\n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            rec_stack.append((v, None))\n",
    "            for (nbr, agent, avail) in graph[v]:\n",
    "                for idx, (node, _) in enumerate(rec_stack):\n",
    "                    if node == nbr:\n",
    "                        cycle_edges = []\n",
    "                        # rec_stack[idx+1:] に記録されているエッジ情報がサイクル内のエッジ\n",
    "                        for j in range(idx + 1, len(rec_stack)):\n",
    "                            edge = rec_stack[j][1]\n",
    "                            if edge is not None:\n",
    "                                cycle_edges.append(edge)\n",
    "                        cycle_edges.append((v, nbr, agent, avail))\n",
    "                        return cycle_edges\n",
    "                if nbr not in visited:\n",
    "                    rec_stack.append((v, (v, nbr, agent, avail)))\n",
    "                    result = dfs(nbr)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "                    rec_stack.pop()\n",
    "            rec_stack.pop()\n",
    "            return None\n",
    "\n",
    "        for vertex in range(self.n):\n",
    "            if vertex not in visited:\n",
    "                cycle = dfs(vertex)\n",
    "                if cycle is not None:\n",
    "                    return cycle\n",
    "        return None\n",
    "\n",
    "    def execute_all_cycles(self):\n",
    "        \"\"\"\n",
    "        現在の二重確率行列 self.P に対して、グラフを構築し、サイクルを探索しては\n",
    "        そのサイクル内で交換可能な最小の確率 epsilon だけ交換を実施する。\n",
    "        複数のサイクルが存在する場合、すべてのサイクルで交換が行われるまで反復する。\n",
    "        交換が行われたサイクルと各サイクルでの epsilon を記録し、最終的な更新後行列 Q と\n",
    "        サイクル情報のリストを返す。\n",
    "        \"\"\"\n",
    "        Q = self.P.clone()\n",
    "        cycles_exchanges = []\n",
    "        ev = 0.0\n",
    "        while True:\n",
    "            graph = self.build_graph(Q)\n",
    "            cycle = self.find_cycle(graph)\n",
    "            if cycle is None:\n",
    "                break\n",
    "            # サイクル内の各エッジの利用可能な確率の最小値を epsilon とする\n",
    "            epsilons = [edge[3] for edge in cycle]\n",
    "            epsilon = min(epsilons)\n",
    "            ev += epsilon\n",
    "            # サイクル内の各エッジについて交換を実施\n",
    "            for (a, b, agent, avail) in cycle:\n",
    "                Q[agent, b] -= epsilon\n",
    "                Q[agent, a] += epsilon\n",
    "            cycles_exchanges.append((cycle, epsilon))\n",
    "        return ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "交換された確率の和: 0.7000000178813934\n"
     ]
    }
   ],
   "source": [
    "# 例：2つの2人サイクルが存在するケース（4エージェント・4オブジェクト）\n",
    "P = torch.tensor([\n",
    "    [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "    [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "    [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ])\n",
    "    \n",
    "# 各行は {1, 0.75, 0.5, 0.25} の順列\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.50, 0.25, 1.00, 0.75],\n",
    "    [0.25, 0.50, 0.75, 1.00]\n",
    "    ])\n",
    "\n",
    "adjuster = compute_ev(P, preferences)\n",
    "\n",
    "sum_epsilon = adjuster.execute_all_cycles()\n",
    "    \n",
    "print(\"\\n交換された確率の和:\", sum_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal treatment of equals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def violation_degree(P, preferences):\n",
    "    \"\"\"\n",
    "    P: (n x n) の確率配分行列（各行はエージェントの割当確率ベクトル）\n",
    "    preferences: (n x m) の選好行列（各行はエージェントの各財に対する評価）\n",
    "    \"\"\"\n",
    "    n = preferences.shape[0]\n",
    "    violation = 0.0\n",
    "    equal_pair_count = 0\n",
    "    \n",
    "    # 各エージェントペアについて、選好が完全に一致していればその差を計算\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            # 全成分が等しければ同等とみなす\n",
    "            if torch.all(preferences[i] == preferences[j]):\n",
    "                diff = torch.sum(torch.abs(P[i] - P[j]))\n",
    "                violation += diff\n",
    "                equal_pair_count += 1\n",
    "    return violation, equal_pair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal treatment of equals の違反度合い: 0.0\n",
      "同一選好のエージェントペア数: 1\n"
     ]
    }
   ],
   "source": [
    "# 例: 選好行列 (4人×4財) \n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.25, 0.50]\n",
    "])\n",
    "\n",
    "# 例: 確率配分行列 (ここでは単純な例として4人とも均等割当)\n",
    "P = torch.tensor([\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.30, 0.20, 0.30, 0.20],\n",
    "    [0.20, 0.30, 0.20, 0.30]\n",
    "])\n",
    "\n",
    "violation, count = violation_degree(P, preferences)\n",
    "print(\"Equal treatment of equals の違反度合い:\", violation.item())\n",
    "print(\"同一選好のエージェントペア数:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal treatment of equals の違反度合い: tensor(0.2000)\n",
      "同一選好のエージェントペア数: 1\n"
     ]
    }
   ],
   "source": [
    "# 選好行列の例 (4人×4財)\n",
    "# エージェント 1 と 2 は同一の選好を持つ\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.25, 0.50]\n",
    "])\n",
    "\n",
    "# 確率配分行列の例\n",
    "# エージェント 1 と 2 は同一選好ですが、割当が異なる\n",
    "P = torch.tensor([\n",
    "    [0.20, 0.30, 0.30, 0.20],  # エージェント 1\n",
    "    [0.25, 0.25, 0.25, 0.25],  # エージェント 2 (同じ選好ながら異なる割当)\n",
    "    [0.30, 0.30, 0.20, 0.20],  # エージェント 3\n",
    "    [0.25, 0.25, 0.25, 0.25]   # エージェント 4\n",
    "])\n",
    "\n",
    "violation, count = violation_degree(P, preferences)\n",
    "print(\"Equal treatment of equals の違反度合い:\", violation)\n",
    "print(\"同一選好のエージェントペア数:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorではなくfloatになるように修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def violation_degree(P, preferences):\n",
    "    \"\"\"\n",
    "    P: (n x n) の確率配分行列。各行はエージェントの割当確率ベクトルを表す。\n",
    "    preferences: (n x m) の選好行列。各行はエージェントの各財に対する評価を表し、\n",
    "                 数値が大きいほど好ましいと解釈される。\n",
    "    \"\"\"\n",
    "    n = preferences.shape[0]\n",
    "    violation = 0.0  # Pythonのfloatとして初期化\n",
    "    equal_pair_count = 0\n",
    "    \n",
    "    # 全てのエージェントペア (i, j) について、選好が完全一致していれば検証\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if torch.all(preferences[i] == preferences[j]):\n",
    "                # L1ノルムを計算し、.item()で数値に変換\n",
    "                diff = torch.sum(torch.abs(P[i] - P[j])).item()\n",
    "                violation += diff\n",
    "                equal_pair_count += 1\n",
    "    return violation, equal_pair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal treatment of equals の違反度合い: 0.20000001788139343\n",
      "同一選好のエージェントペア数: 1\n"
     ]
    }
   ],
   "source": [
    "# 例: 選好行列 (4人×4財)\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.25, 0.50]\n",
    "])\n",
    "\n",
    "# 例: 確率配分行列\n",
    "# エージェント 1 と 2 は同一の選好ですが、割当が異なるため違反度合いが発生\n",
    "P = torch.tensor([\n",
    "    [0.20, 0.30, 0.30, 0.20],  # エージェント 1\n",
    "    [0.25, 0.25, 0.25, 0.25],  # エージェント 2 (同じ選好ながら異なる割当)\n",
    "    [0.30, 0.30, 0.20, 0.20],  # エージェント 3\n",
    "    [0.25, 0.25, 0.25, 0.25]   # エージェント 4\n",
    "])\n",
    "\n",
    "violation, count = violation_degree(P, preferences)\n",
    "print(\"Equal treatment of equals の違反度合い:\", violation)  # 単なる数値が出力される\n",
    "print(\"同一選好のエージェントペア数:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal treatment of equals の違反度合い: 0.40000003576278687\n",
      "同一選好のエージェントペア数: 2\n"
     ]
    }
   ],
   "source": [
    "# 選好行列の例 (4 人×4 財)\n",
    "# グループ 1: エージェント 0 と 1 の選好は同一\n",
    "# グループ 2: エージェント 2 と 3 の選好は同一（ただし、グループ 1 とは異なる）\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],  # エージェント 0\n",
    "    [1.00, 0.75, 0.50, 0.25],  # エージェント 1\n",
    "    [0.75, 1.00, 0.50, 0.25],  # エージェント 2\n",
    "    [0.75, 1.00, 0.50, 0.25]   # エージェント 3\n",
    "])\n",
    "\n",
    "# 確率配分行列の例 (4人×4財)\n",
    "# 同一選好のグループ内で割当が異なるため、違反度合いが発生します。\n",
    "P = torch.tensor([\n",
    "    [0.20, 0.30, 0.30, 0.20],  # エージェント 0\n",
    "    [0.25, 0.25, 0.25, 0.25],  # エージェント 1 (グループ 1内で異なる)\n",
    "    [0.30, 0.30, 0.20, 0.20],  # エージェント 2\n",
    "    [0.25, 0.25, 0.25, 0.25]   # エージェント 3 (グループ 2内で異なる)\n",
    "])\n",
    "\n",
    "violation, count = violation_degree(P, preferences)\n",
    "print(\"Equal treatment of equals の違反度合い:\", violation)\n",
    "print(\"同一選好のエージェントペア数:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal treatment of equals の違反度合い: 0.6000000536441803\n",
      "同一選好のエージェントペア数: 3\n"
     ]
    }
   ],
   "source": [
    "# 選好行列 (4人×4財)\n",
    "# エージェント0,1,2は同一の選好 [1.00, 0.75, 0.50, 0.25] を持ち、\n",
    "# エージェント3は異なる選好 [0.75, 1.00, 0.25, 0.50] を持つ\n",
    "preferences = torch.tensor([\n",
    "    [1.00, 0.75, 0.50, 0.25],  # エージェント0\n",
    "    [1.00, 0.75, 0.50, 0.25],  # エージェント1\n",
    "    [1.00, 0.75, 0.50, 0.25],  # エージェント2\n",
    "    [0.75, 1.00, 0.25, 0.50]   # エージェント3 (異なる選好)\n",
    "])\n",
    "\n",
    "# 確率配分行列 (4×4)\n",
    "# エージェント0,1,2は同一選好ですが、割当が異なるため違反度合いが発生\n",
    "P = torch.tensor([\n",
    "    [0.30, 0.30, 0.20, 0.20],  # エージェント0\n",
    "    [0.25, 0.25, 0.25, 0.25],  # エージェント1\n",
    "    [0.20, 0.30, 0.30, 0.20],  # エージェント2\n",
    "    [0.25, 0.25, 0.25, 0.25]   # エージェント3\n",
    "])\n",
    "\n",
    "violation, count = violation_degree(P, preferences)\n",
    "print(\"Equal treatment of equals の違反度合い:\", violation)  # 期待される出力: 0.60\n",
    "print(\"同一選好のエージェントペア数:\", count)               # 期待される出力: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal treatment of equals の違反度合い: 2.600000113248825\n"
     ]
    }
   ],
   "source": [
    "# 4人全員が同一の選好 [1.0, 0.75, 0.5, 0.25] を持つ\n",
    "preferences = torch.tensor([\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25]\n",
    "])\n",
    "\n",
    "# 確率配分行列 (4人×4オブジェクト)\n",
    "P = torch.tensor([\n",
    "    [0.4,  0.3,  0.2,  0.1],   # エージェント0\n",
    "    [0.3,  0.3,  0.2,  0.2],   # エージェント1\n",
    "    [0.25, 0.25, 0.25, 0.25],   # エージェント2\n",
    "    [0.1,  0.2,  0.3,  0.4]     # エージェント3\n",
    "])\n",
    "\n",
    "violation = violation_degree(P, preferences)\n",
    "print(\"Equal treatment of equals の違反度合い:\", violation)  # 期待される出力: 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "violationのみ出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def violation_degree(P, preferences):\n",
    "    \"\"\"\n",
    "    P: (n x n) の確率配分行列。各行はエージェントの割当確率ベクトルを表す。\n",
    "    preferences: (n x m) の選好行列。各行はエージェントの各財に対する評価を表し、\n",
    "                 数値が大きいほど好ましいと解釈される。\n",
    "    \"\"\"\n",
    "    n = preferences.shape[0]\n",
    "    violation = 0.0  # Pythonのfloatとして初期化\n",
    "    \n",
    "    # 全てのエージェントペア (i, j) について、選好が完全一致していれば検証\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if torch.all(preferences[i] == preferences[j]):\n",
    "                # L1ノルムを計算し、.item()で数値に変換\n",
    "                diff = torch.sum(torch.abs(P[i] - P[j])).item()\n",
    "                violation += diff\n",
    "    return violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def violation_degree(P, preferences):\n",
    "    \"\"\"\n",
    "    P: (n x n) の確率配分行列（各行はエージェントの割当確率ベクトル）\n",
    "    preferences: (n x m) の選好行列（各行はエージェントの各財に対する評価）\n",
    "    \"\"\"\n",
    "    n = preferences.shape[0]\n",
    "    violation = 0.0\n",
    "    equal_pair_count = 0\n",
    "    \n",
    "    # 各エージェントペアについて、選好が完全に一致していればその差を計算\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            # 全成分が等しければ同等とみなす\n",
    "            if torch.all(preferences[i] == preferences[j]):\n",
    "                diff = torch.sum(torch.abs(P[i] - P[j]))\n",
    "                violation += diff\n",
    "                equal_pair_count += 1\n",
    "    return violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal treatment of equals の違反度合い: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "preferences = torch.tensor([[\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25]\n",
    "],[\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25]\n",
    "]])\n",
    "\n",
    "# 確率配分行列 (4人×4オブジェクト)\n",
    "P = torch.tensor([[\n",
    "    [0.4,  0.3,  0.2,  0.1],   # エージェント0\n",
    "    [0.3,  0.3,  0.2,  0.2],   # エージェント1\n",
    "    [0.25, 0.25, 0.25, 0.25],   # エージェント2\n",
    "    [0.1,  0.2,  0.3,  0.4]     # エージェント3\n",
    "],[\n",
    "    [0.4,  0.3,  0.2,  0.1],   # エージェント0\n",
    "    [0.3,  0.3,  0.2,  0.2],   # エージェント1\n",
    "    [0.25, 0.25, 0.25, 0.25],   # エージェント2\n",
    "    [0.1,  0.2,  0.3,  0.4]     # エージェント3\n",
    "]])\n",
    "\n",
    "violation = violation_degree(P, preferences)\n",
    "print(\"Equal treatment of equals の違反度合い:\", violation)  # 期待される出力: 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class compute_ev:\n",
    "    def __init__(self, P, preferences):\n",
    "        \"\"\"\n",
    "        P: n×n の二重確率行列 (torch.Tensor)\n",
    "        preferences: n×n の選好行列 (torch.Tensor)\n",
    "                     各行 i はエージェント i の選好を表し、値が大きいほど好む\n",
    "        \"\"\"\n",
    "        self.P = P.clone()\n",
    "        self.preferences = preferences\n",
    "        self.n = P.shape[1]\n",
    "\n",
    "    def build_graph(self, Q):\n",
    "        graph = {a: [] for a in range(self.n)}\n",
    "        for i in range(self.n):\n",
    "            sorted_preferences = torch.argsort(self.preferences[i], descending=True)\n",
    "            for a in sorted_preferences:\n",
    "                for b in sorted_preferences:\n",
    "                    if a == b:\n",
    "                        continue\n",
    "                    if Q[i, b] > 0:\n",
    "                        if a not in graph:\n",
    "                            graph[a] = []\n",
    "                        graph[a].append((b, i, Q[i, b].item()))\n",
    "                        break\n",
    "        return graph\n",
    "    \n",
    "    def find_cycle(self, graph):\n",
    "        visited = set()\n",
    "        rec_stack = []\n",
    "\n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            rec_stack.append((v, None))\n",
    "            for (nbr, agent, avail) in graph[v]:\n",
    "                if nbr in visited:\n",
    "                    cycle_edges = [(v, nbr, agent, avail)]\n",
    "                    for node, edge in reversed(rec_stack):\n",
    "                        if node == nbr:\n",
    "                            break\n",
    "                        cycle_edges.append(edge)\n",
    "                    return cycle_edges\n",
    "                if nbr not in visited:\n",
    "                    rec_stack.append((v, (v, nbr, agent, avail)))\n",
    "                    result = dfs(nbr)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "                    rec_stack.pop()\n",
    "            rec_stack.pop()\n",
    "            return None\n",
    "\n",
    "        for vertex in range(self.n):\n",
    "            if vertex not in visited:\n",
    "                cycle = dfs(vertex)\n",
    "                if cycle is not None:\n",
    "                    return cycle\n",
    "        return None\n",
    "    \n",
    "    def execute_all_cycles(self):\n",
    "        Q = self.P.clone()\n",
    "        cycles_exchanges = []\n",
    "        violation = 0.0\n",
    "        while True:\n",
    "            graph = self.build_graph(Q)\n",
    "            cycle = self.find_cycle(graph)\n",
    "            if cycle is None:\n",
    "                break\n",
    "            epsilons = [edge[3] for edge in cycle]\n",
    "            epsilon = min(epsilons)\n",
    "            violation += epsilon\n",
    "            for (a, b, agent, avail) in cycle:\n",
    "                Q[agent, b] -= epsilon\n",
    "                Q[agent, a] += epsilon\n",
    "            cycles_exchanges.append((cycle, epsilon))\n",
    "        return violation\n",
    "\n",
    "    def execute_all_cycles_batch(self):\n",
    "        \"\"\"\n",
    "        P と preferences の各バッチに対して execute_all_cycles を計算し、\n",
    "        それらの結果を合わせて n*1 の行列にする。\n",
    "        \"\"\"\n",
    "        batch_size = self.P.shape[0]\n",
    "        results = torch.zeros((batch_size, 1), dtype=torch.float32)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            P_batch = self.P[b].view(self.n, self.n)\n",
    "            preferences_batch = self.preferences[b].view(self.n, self.n)\n",
    "            ev_instance = compute_ev(P_batch, preferences_batch)\n",
    "            results[b, 0] = ev_instance.execute_all_cycles()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class compute_ev:\n",
    "    def __init__(self, P, preferences):\n",
    "        \"\"\"\n",
    "        P: n×n の二重確率行列 (torch.Tensor)\n",
    "        preferences: n×n の選好行列 (torch.Tensor)\n",
    "                     各行 i はエージェント i の選好を表し、値が大きいほど好む\n",
    "        \"\"\"\n",
    "        self.P = P.clone()\n",
    "        self.preferences = preferences\n",
    "        self.n = P.shape[1]\n",
    "\n",
    "    def build_graph(self, Q):\n",
    "        \"\"\"\n",
    "        Q: 現在の二重確率行列 (torch.Tensor)\n",
    "        オブジェクトを頂点とするグラフを構築する。\n",
    "        エッジ (a -> b) は、あるエージェント i が a を b より好む（preferences[i, a] > preferences[i, b]）\n",
    "        かつ Q[i, b] > 0 である場合に追加する。\n",
    "        エッジには (b, i, Q[i, b]) の情報を記録する。\n",
    "        \"\"\"\n",
    "        graph = {a: [] for a in range(self.n)}\n",
    "        for a in range(self.n):\n",
    "            for b in range(self.n):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                for i in range(self.n):\n",
    "                    if self.preferences[i, a] > self.preferences[i, b] and Q[i, b] > 0:\n",
    "                        graph[a].append((b, i, Q[i, b].item())) \n",
    "                        # 同じ (a, b) ペアについて、最初に条件を満たしたエージェントを witness とする\n",
    "                        break\n",
    "        return graph\n",
    "\n",
    "    def find_cycle(self, graph):\n",
    "        \"\"\"\n",
    "        DFS を用いてグラフ中のサイクル（閉路）を探索する。\n",
    "        サイクルが見つかった場合は、サイクルを構成するエッジのリストを返す。\n",
    "        各エッジは (from_object, to_object, witness_agent, available_probability) のタプル。\n",
    "        サイクルがなければ None を返す。\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = []  # 各要素は (vertex, edge_info)。最初の頂点は edge_info=None\n",
    "\n",
    "        def dfs(v):\n",
    "            visited.add(v)\n",
    "            rec_stack.append((v, None))\n",
    "            for (nbr, agent, avail) in graph[v]:\n",
    "                for idx, (node, _) in enumerate(rec_stack):\n",
    "                    if node == nbr:\n",
    "                        cycle_edges = []\n",
    "                        # rec_stack[idx+1:] に記録されているエッジ情報がサイクル内のエッジ\n",
    "                        for j in range(idx + 1, len(rec_stack)):\n",
    "                            edge = rec_stack[j][1]\n",
    "                            if edge is not None:\n",
    "                                cycle_edges.append(edge)\n",
    "                        cycle_edges.append((v, nbr, agent, avail))\n",
    "                        return cycle_edges\n",
    "                if nbr not in visited:\n",
    "                    rec_stack.append((v, (v, nbr, agent, avail)))\n",
    "                    result = dfs(nbr)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "                    rec_stack.pop()\n",
    "            rec_stack.pop()\n",
    "            return None\n",
    "\n",
    "        for vertex in range(self.n):\n",
    "            if vertex not in visited:\n",
    "                cycle = dfs(vertex)\n",
    "                if cycle is not None:\n",
    "                    return cycle\n",
    "        return None\n",
    "\n",
    "    def execute_all_cycles(self):\n",
    "        \"\"\"\n",
    "        現在の二重確率行列 self.P に対して、グラフを構築し、サイクルを探索しては\n",
    "        そのサイクル内で交換可能な最小の確率 epsilon だけ交換を実施する。\n",
    "        複数のサイクルが存在する場合、すべてのサイクルで交換が行われるまで反復する。\n",
    "        交換が行われたサイクルと各サイクルでの epsilon を記録し、最終的な更新後行列 Q と\n",
    "        サイクル情報のリストを返す。\n",
    "        \"\"\"\n",
    "        Q = self.P.clone()\n",
    "        cycles_exchanges = []\n",
    "        violation = 0.0\n",
    "        while True:\n",
    "            graph = self.build_graph(Q)\n",
    "            cycle = self.find_cycle(graph)\n",
    "            if cycle is None:\n",
    "                break\n",
    "            # サイクル内の各エッジの利用可能な確率の最小値を epsilon とする\n",
    "            epsilons = [edge[3] for edge in cycle]\n",
    "            epsilon = min(epsilons)\n",
    "            violation += epsilon\n",
    "            # サイクル内の各エッジについて交換を実施\n",
    "            for (a, b, agent, avail) in cycle:\n",
    "                Q[agent, b] -= epsilon\n",
    "                Q[agent, a] += epsilon\n",
    "            cycles_exchanges.append((cycle, epsilon))\n",
    "        return violation\n",
    "\n",
    "    def execute_all_cycles_batch(self):\n",
    "        \"\"\"\n",
    "        P と preferences の各バッチに対して execute_all_cycles を計算し、\n",
    "        それらの結果を合わせて n*1 の行列にする。\n",
    "        \"\"\"\n",
    "        batch_size = self.P.shape[0]\n",
    "        results = torch.zeros((batch_size, 1), dtype=torch.float32)\n",
    "\n",
    "        def process_batch(b):\n",
    "            P_batch = self.P[b].view(self.n, self.n)\n",
    "            preferences_batch = self.preferences[b].view(self.n, self.n)\n",
    "            ev_instance = compute_ev(P_batch, preferences_batch)\n",
    "            return ev_instance.execute_all_cycles()\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results_list = list(executor.map(process_batch, range(batch_size)))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            results[b, 0] = results_list[b]\n",
    "\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "preferences = torch.tensor([[\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25]\n",
    "],[\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25],\n",
    "    [1.0, 0.75, 0.5, 0.25]\n",
    "]])\n",
    "\n",
    "# 確率配分行列 (4人×4オブジェクト)\n",
    "P = torch.tensor([[\n",
    "    [0.4,  0.3,  0.2,  0.1],   # エージェント0\n",
    "    [0.3,  0.3,  0.2,  0.2],   # エージェント1\n",
    "    [0.25, 0.25, 0.25, 0.25],   # エージェント2\n",
    "    [0.1,  0.2,  0.3,  0.4]     # エージェント3\n",
    "],[\n",
    "    [0.4,  0.3,  0.2,  0.1],   # エージェント0\n",
    "    [0.3,  0.3,  0.2,  0.2],   # エージェント1\n",
    "    [0.25, 0.25, 0.25, 0.25],   # エージェント2\n",
    "    [0.1,  0.2,  0.3,  0.4]     # エージェント3\n",
    "]])\n",
    "\n",
    "adjuster = compute_ev(P, preferences)\n",
    "print(adjuster.execute_all_cycles_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7000],\n",
      "        [0.7000]])\n"
     ]
    }
   ],
   "source": [
    "# 例：2つの2人サイクルが存在するケース（4エージェント・4オブジェクト）\n",
    "P = torch.tensor([[\n",
    "    [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "    [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "    [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ],[\n",
    "    [0.7, 0.3, 0.0, 0.0],  # Agent0\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Agent1\n",
    "    [0.0, 0.0, 0.6, 0.4],  # Agent2\n",
    "    [0.0, 0.0, 0.4, 0.6]   # Agent3\n",
    "    ]])\n",
    "    \n",
    "# 各行は {1, 0.75, 0.5, 0.25} の順列\n",
    "preferences = torch.tensor([[\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.50, 0.25, 1.00, 0.75],\n",
    "    [0.25, 0.50, 0.75, 1.00]\n",
    "    ],[\n",
    "    [1.00, 0.75, 0.50, 0.25],\n",
    "    [0.75, 1.00, 0.50, 0.25],\n",
    "    [0.50, 0.25, 1.00, 0.75],\n",
    "    [0.25, 0.50, 0.75, 1.00]\n",
    "    ]])\n",
    "\n",
    "adjuster = compute_ev(P, preferences)\n",
    "print(adjuster.execute_all_cycles_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2000],\n",
      "        [1.2000]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.tensor([[\n",
    "        [0.4, 0.3, 0.0, 0.3],\n",
    "        [0.0, 0.4, 0.3, 0.3],\n",
    "        [0.3, 0.3, 0.4, 0.0],\n",
    "        [0.3, 0.0, 0.3, 0.4]\n",
    "    ],[\n",
    "        [0.4, 0.3, 0.0, 0.3],\n",
    "        [0.0, 0.4, 0.3, 0.3],\n",
    "        [0.3, 0.3, 0.4, 0.0],\n",
    "        [0.3, 0.0, 0.3, 0.4]\n",
    "    ]])\n",
    "\n",
    "preferences = torch.tensor([[\n",
    "        [1.00, 0.75, 0.5,  0.25],   # Agent0\n",
    "        [0.75, 1.00, 0.5,  0.25],   # Agent1\n",
    "        [0.75, 0.5,  1.00, 0.25],   # Agent2\n",
    "        [0.75, 0.5,  0.25, 1.00]     # Agent3\n",
    "    ],[\n",
    "        [1.00, 0.75, 0.5,  0.25],   # Agent0\n",
    "        [0.75, 1.00, 0.5,  0.25],   # Agent1\n",
    "        [0.75, 0.5,  1.00, 0.25],   # Agent2\n",
    "        [0.75, 0.5,  0.25, 1.00]     # Agent3\n",
    "    ]])\n",
    "\n",
    "adjuster = compute_ev(P, preferences)\n",
    "print(adjuster.execute_all_cycles_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2000]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.tensor([[\n",
    "        [0.4, 0.3, 0.0, 0.3],\n",
    "        [0.0, 0.4, 0.3, 0.3],\n",
    "        [0.3, 0.3, 0.4, 0.0],\n",
    "        [0.3, 0.0, 0.3, 0.4]\n",
    "    ]])\n",
    "\n",
    "preferences = torch.tensor([[\n",
    "        [1.00, 0.75, 0.5,  0.25],   # Agent0\n",
    "        [0.75, 1.00, 0.5,  0.25],   # Agent1\n",
    "        [0.75, 0.5,  1.00, 0.25],   # Agent2\n",
    "        [0.75, 0.5,  0.25, 1.00]     # Agent3\n",
    "    ]])\n",
    "\n",
    "adjuster = compute_ev(P, preferences)\n",
    "print(adjuster.execute_all_cycles_batch())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
